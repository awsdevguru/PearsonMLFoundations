{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXeEqNncz+xYDpehVklyQE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awsdevguru/PearsonMLFoundations/blob/main/2_3_05_Sklearn_Pipelines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scikit-learn Pipelines\n",
        "\n",
        "## 1. Objectives\n",
        "* Understand how to chain preprocessing and modeling steps in one workflow\n",
        "* Learn to prevent data leakage by fitting transformations only on training data\n",
        "* Build, tune, and save a reproducible ML pipeline\n",
        "\n",
        "## 2. Setup"
      ],
      "metadata": {
        "id": "EG5GcxzqxH9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib"
      ],
      "metadata": {
        "id": "carriqokxHvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load and Inspect Data\n",
        "\n",
        "Use a simple dataset (e.g., Titanic or a small synthetic dataset):"
      ],
      "metadata": {
        "id": "YtC0ciJBxk9z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhiMEBQUxEKV"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "titanic = fetch_openml('titanic', version=1, as_frame=True)\n",
        "df = titanic.frame[['pclass', 'sex', 'age', 'fare', 'survived']].dropna()\n",
        "X = df[['pclass', 'sex', 'age', 'fare']]\n",
        "y = df['survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=73)\n",
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define Preprocessing\n",
        "\n",
        "* Numerical data -> scaled\n",
        "* Categorical data -> one-hot encoded\n",
        "* All transformations fit only on training data"
      ],
      "metadata": {
        "id": "d_BRUIc5xrnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = ['age', 'fare']\n",
        "cat_features = ['pclass', 'sex']\n",
        "\n",
        "num_transformer = Pipeline([('scaler', StandardScaler())])\n",
        "cat_transformer = Pipeline([('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', num_transformer, num_features),\n",
        "    ('cat', cat_transformer, cat_features)\n",
        "])\n",
        "preprocessor"
      ],
      "metadata": {
        "id": "9ctT5Juvxn42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Build the Pipeline"
      ],
      "metadata": {
        "id": "iNILU-yIx4kC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('model', LogisticRegression(max_iter=1000))\n",
        "])\n",
        "clf"
      ],
      "metadata": {
        "id": "iJhCffgAx2jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "yoPI9z_e9dYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Grid Search with Pipelines\n",
        "\n",
        "Hyperparameter tuning works end-to-end, including preprocessing.\n",
        "\n",
        "This code is doing hyperparameter tuning using Grid Search to find the best settings for a machine learning model inside a pipeline."
      ],
      "metadata": {
        "id": "ilPJndEryEXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'model__C': [0.1, 1.0, 10],\n",
        "    'model__penalty': ['l2']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(clf, param_grid, cv=3)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Test accuracy:\", accuracy_score(y_test, grid.predict(X_test)))"
      ],
      "metadata": {
        "id": "QTd39q3wx7Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Saving and Loading Pipelines"
      ],
      "metadata": {
        "id": "AjBFHcw8yKx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(grid.best_estimator_, 'titanic_pipeline.pkl')\n",
        "loaded_model = joblib.load('titanic_pipeline.pkl')\n",
        "print(\"Reloaded Accuracy:\", accuracy_score(y_test, loaded_model.predict(X_test)))"
      ],
      "metadata": {
        "id": "Wu6YuKEhyM7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Single Prediction"
      ],
      "metadata": {
        "id": "Vsd8SuS29vph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a single, realistic passenger row\n",
        "one = pd.DataFrame([{\n",
        "    \"pclass\": 2,\n",
        "    \"sex\": \"female\",\n",
        "    \"age\": 28,\n",
        "    \"fare\": 20.0\n",
        "}])\n",
        "\n",
        "# predict from loaded model\n",
        "print(\"Survival probability:\", loaded_model.predict_proba(one)[0,1])\n",
        "print(\"Predicted class     :\", int(loaded_model.predict(one)[0]))"
      ],
      "metadata": {
        "id": "X8UaPNhj9x6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Key Takeaways\n",
        "\n",
        "* Pipelines ensure consistent preprocessing and reproducible training.\n",
        "* They simplify code and prevent leakage automatically.\n",
        "* Pipelines are production-ready and easy to deploy with joblib.\n",
        "* You can tune preprocessing and model hyperparameters together."
      ],
      "metadata": {
        "id": "aPcDrtrzyTZf"
      }
    }
  ]
}