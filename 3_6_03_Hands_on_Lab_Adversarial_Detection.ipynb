{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN1PodGlYk4z5X7JgvAZ1UZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awsdevguru/PearsonMLFoundations/blob/main/3_6_03_Hands_on_Lab_Adversarial_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Attacks on CIFAR-10 with FGSM (Fast Gradient Sign Method)\n",
        "\n",
        "This notebook demonstrates how to craft adversarial examples on the CIFAR-10 dataset using the Fast Gradient Sign Method (FGSM) against a simple convolutional neural network (CNN) trained with PyTorch.\n",
        "\n",
        "**Goals**\n",
        "\n",
        "- Load CIFAR-10 and build a simple CNN classifier\n",
        "- Train (or load) the model\n",
        "- Evaluate on clean test images\n",
        "- Implement FGSM and generate adversarial examples\n",
        "- Measure adversarial accuracy vs. perturbation strength (epsilon)\n",
        "- Visualize original vs. adversarial images\n",
        "\n",
        "FGSM is a white-box adversarial attack that perturbs the input image in the\n",
        "direction of the sign of the gradient of the loss with respect to the input. Even small perturbations (often imperceptible) can drastically change the model's prediction.\n",
        "\n",
        "## 1. Imports and Configuration"
      ],
      "metadata": {
        "id": "AK_S9IkrQhG3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j3cTgsQQKsP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# CIFAR-10 normalization stats\n",
        "cifar10_mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1)\n",
        "cifar10_std = torch.tensor([0.2470, 0.2435, 0.2616]).view(3, 1, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load the dataset\n",
        "\n",
        "We will:\n",
        "\n",
        "- Apply standard CIFAR-10 normalization\n",
        "- Use basic data augmentation (random crop + horizontal flip) for training\n",
        "- Build PyTorch DataLoaders for train and test sets\n",
        "\n",
        "For faster experimentation, you can optionally subsample the training set."
      ],
      "metadata": {
        "id": "Sc9w73CfQzmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms for training and test sets\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        (0.4914, 0.4822, 0.4465),\n",
        "        (0.2470, 0.2435, 0.2616),\n",
        "    ),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        (0.4914, 0.4822, 0.4465),\n",
        "        (0.2470, 0.2435, 0.2616),\n",
        "    ),\n",
        "])\n",
        "\n",
        "# Download CIFAR-10\n",
        "trainset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=True, download=True, transform=transform_train\n",
        ")\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root=\"./data\", train=False, download=True, transform=transform_test\n",
        ")\n",
        "\n",
        "# Optional: subsample training set for quicker runs\n",
        "train_subset_size = 10_000   # set to None to use full dataset\n",
        "\n",
        "if train_subset_size is not None:\n",
        "    indices = np.random.permutation(len(trainset))[:train_subset_size]\n",
        "    trainset = Subset(trainset, indices)\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "print(\"Train size:\", len(trainset))\n",
        "print(\"Test size:\", len(testset))\n"
      ],
      "metadata": {
        "id": "NcX_Mn6HQqlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define a simple CNN classifier for CIFAR-10\n",
        "\n",
        "This is a small convolutional network, not meant to be state-of-the-art,\n",
        "but good enough to illustrate adversarial attacks.\n"
      ],
      "metadata": {
        "id": "yCkx4fNlSg2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCIFAR10CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))   # 32x32 -> 16x16\n",
        "        x = self.pool(F.relu(self.conv2(x)))   # 16x16 -> 8x8\n",
        "        x = self.pool(F.relu(self.conv3(x)))   # 8x8  -> 4x4\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCIFAR10CNN().to(device)\n",
        "print(model)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "checkpoint_path = Path(\"cifar10_simplecnn_fgsm_demo.pth\")\n"
      ],
      "metadata": {
        "id": "OYg_wo9FQ7Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train (or load) the model\n",
        "\n",
        "To keep the notebook flexible:\n",
        "- If a checkpoint exists, we load it.\n",
        "- Otherwise, we train for a few epochs and save the model.\n",
        "\n",
        "Increase the number of epochs for better accuracy (and stronger demonstrations).\n"
      ],
      "metadata": {
        "id": "I5K61Su4Sl-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, trainloader, optimizer, criterion, device, epochs=5):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in trainloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss = running_loss / len(trainloader)\n",
        "        epoch_acc = 100.0 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - acc: {epoch_acc:.2f}%\")\n",
        "    print(\"Finished Training\")\n",
        "\n",
        "\n",
        "def evaluate_accuracy(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return 100.0 * correct / total\n"
      ],
      "metadata": {
        "id": "h1zBYet_RRWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train or Load"
      ],
      "metadata": {
        "id": "9M5S3KbgSt3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_MODEL = True  # set False if you only want to load an existing checkpoint\n",
        "\n",
        "if checkpoint_path.exists():\n",
        "    print(f\"Loading existing checkpoint from {checkpoint_path}\")\n",
        "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "else:\n",
        "    if TRAIN_MODEL:\n",
        "        print(\"No checkpoint found. Training model...\")\n",
        "        train(model, trainloader, optimizer, criterion, device, epochs=5)\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        print(f\"Saved checkpoint to {checkpoint_path}\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"Checkpoint not found and TRAIN_MODEL is False.\")\n",
        "\n",
        "clean_acc = evaluate_accuracy(model, testloader, device)\n",
        "print(f\"Clean test accuracy: {clean_acc:.2f}%\")\n"
      ],
      "metadata": {
        "id": "QspYqdaERUGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Implementing FGSM\n",
        "\n",
        "For each input image:\n",
        "\n",
        "1. Enable gradient tracking on the input.\n",
        "2. Compute the loss for the correct labels.\n",
        "3. Backpropagate to get the gradient of the loss with respect to the input.\n",
        "4. Take the sign of this gradient.\n",
        "5. Create an adversarial example by adding a small step in the direction of the sign.\n",
        "\n",
        "We clamp the perturbed image back to a reasonable range to avoid numerical issues.\n",
        "Because we are working in *normalized* pixel space, the bound is not [0, 1]; instead\n",
        "we clamp to a broad range (e.g. [-3, 3]) that comfortably covers CIFAR-10 after normalization.\n"
      ],
      "metadata": {
        "id": "xahq5DO6RaNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fgsm_attack(model, images, labels, epsilon, device, clamp_min=-3.0, clamp_max=3.0):\n",
        "    \"\"\"\n",
        "    Perform FGSM attack on a batch of images.\n",
        "\n",
        "    Args:\n",
        "        model: trained classifier\n",
        "        images: normalized input images (B, C, H, W)\n",
        "        labels: ground-truth labels\n",
        "        epsilon: size of perturbation\n",
        "        device: torch device\n",
        "        clamp_min, clamp_max: bounds in normalized space\n",
        "\n",
        "    Returns:\n",
        "        adv_images: adversarial images\n",
        "    \"\"\"\n",
        "    images = images.clone().detach().to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    images.requires_grad = True\n",
        "\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Sign of the gradient\n",
        "    grad_sign = images.grad.data.sign()\n",
        "\n",
        "    # Create perturbed image\n",
        "    adv_images = images + epsilon * grad_sign\n",
        "    adv_images = torch.clamp(adv_images, clamp_min, clamp_max)\n",
        "\n",
        "    return adv_images.detach()\n"
      ],
      "metadata": {
        "id": "Aw3Qdyr3RXsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Evaluating adversarial robustness vs. epsilon\n",
        "\n",
        "We now:\n",
        "- Loop over a set of epsilon values\n",
        "- For each epsilon, craft adversarial examples on the fly for the entire test set\n",
        "- Measure the resulting adversarial accuracy\n",
        "\n",
        "This shows how accuracy degrades as the attack strength increases.\n"
      ],
      "metadata": {
        "id": "vti-toUMRdl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_fgsm(model, dataloader, device, epsilons):\n",
        "    model.eval()\n",
        "    epsilon_to_acc = {}\n",
        "\n",
        "    for eps in epsilons:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Generate adversarial examples\n",
        "            adv_inputs = fgsm_attack(model, inputs, labels, epsilon=eps, device=device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(adv_inputs)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epsilon_to_acc[eps] = 100.0 * correct / total\n",
        "        print(f\"Epsilon {eps:.3f} -> adversarial accuracy: {epsilon_to_acc[eps]:.2f}%\")\n",
        "\n",
        "    return epsilon_to_acc\n",
        "\n",
        "\n",
        "epsilons = [0.0, 0.01, 0.03, 0.05, 0.1]\n",
        "epsilon_to_acc = evaluate_fgsm(model, testloader, device, epsilons)\n"
      ],
      "metadata": {
        "id": "S3hL5PiVRb-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Plot adversarial accuracy vs. epsilon\n",
        "\n",
        "Let's visualize how the model's accuracy changes as we increase the perturbation size.\n"
      ],
      "metadata": {
        "id": "dgz-wZfJRg3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "eps_list = sorted(epsilon_to_acc.keys())\n",
        "acc_list = [epsilon_to_acc[eps] for eps in eps_list]\n",
        "\n",
        "plt.plot(eps_list, acc_list, marker=\"o\")\n",
        "plt.xlabel(\"Epsilon (FGSM perturbation size)\")\n",
        "plt.ylabel(\"Accuracy on adversarial examples (%)\")\n",
        "plt.title(\"CIFAR-10 FGSM Attack: Accuracy vs. Epsilon\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2tM_2wciRfKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Visualizing original vs. adversarial images\n",
        "\n",
        "To build intuition, we'll:\n",
        "- Take a small batch from the test set,\n",
        "- Generate adversarial examples,\n",
        "- Undo the normalization,\n",
        "- Show clean vs. adversarial images side by side with predicted labels.\n"
      ],
      "metadata": {
        "id": "13QkHhuiRkWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpers to unnormalize and display images\n",
        "\n",
        "mean = torch.tensor(cifar10_mean).view(3, 1, 1)\n",
        "std = torch.tensor(cifar10_std).view(3, 1, 1)\n",
        "\n",
        "def unnormalize(img_tensor):\n",
        "    \"\"\"\n",
        "    img_tensor: (C, H, W) in normalized space\n",
        "    returns: (C, H, W) in [0, 1]\n",
        "    \"\"\"\n",
        "    return torch.clamp(img_tensor * std + mean, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def show_images(clean_imgs, adv_imgs, clean_labels, adv_preds, classes, n=5):\n",
        "    \"\"\"\n",
        "    Show n examples of original vs. adversarial images.\n",
        "    \"\"\"\n",
        "    n = min(n, clean_imgs.size(0))\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    for i in range(n):\n",
        "        # Clean\n",
        "        plt.subplot(2, n, i + 1)\n",
        "        img = unnormalize(clean_imgs[i].cpu())\n",
        "        plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(classes[clean_labels[i].item()])\n",
        "\n",
        "        # Adversarial\n",
        "        plt.subplot(2, n, n + i + 1)\n",
        "        img_adv = unnormalize(adv_imgs[i].cpu())\n",
        "        plt.imshow(np.transpose(img_adv.numpy(), (1, 2, 0)))\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(classes[adv_preds[i].item()])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "AUtezMWBRlx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick one batch from the test set\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "epsilon_demo = 0.03  # choose a moderate epsilon\n",
        "\n",
        "# Generate adversarial batch\n",
        "adv_images = fgsm_attack(model, images, labels, epsilon=epsilon_demo, device=device)\n",
        "\n",
        "# Get predictions for adversarial images\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs_adv = model(adv_images)\n",
        "    _, preds_adv = outputs_adv.max(1)\n",
        "\n",
        "print(f\"Showing first few images for epsilon = {epsilon_demo}\")\n",
        "\n",
        "show_images(images, adv_images, labels, preds_adv, classes, n=6)\n"
      ],
      "metadata": {
        "id": "WQ4Zs1hVRoD8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}