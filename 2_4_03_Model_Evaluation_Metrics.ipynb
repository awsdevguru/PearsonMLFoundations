{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYG9pnlgOUNWSBIgfFZATh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/awsdevguru/PearsonMLFoundations/blob/main/2_4_03_Model_Evaluation_Metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation Metrics\n",
        "\n",
        "Goal: practice computing and interpreting common classification metrics, see threshold trade-offs, and learn which metric to use when. Uses scikit-learn with a slightly imbalanced dataset."
      ],
      "metadata": {
        "id": "TfkRM5ytTYw6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6_osYwxTO_R"
      },
      "outputs": [],
      "source": [
        "# Core\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ML\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_validate\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    roc_auc_score, RocCurveDisplay, average_precision_score, PrecisionRecallDisplay,\n",
        "    precision_recall_curve, auc, balanced_accuracy_score\n",
        ")\n",
        "\n",
        "# Reproducible synthetic, imbalanced data\n",
        "X, y = make_classification(\n",
        "    n_samples=4000, n_features=20, n_informative=6, n_redundant=3,\n",
        "    weights=[0.9, 0.1], flip_y=0.01, class_sep=1.0, random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Simple, robust baseline classifier\n",
        "clf = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"model\", LogisticRegression(max_iter=1000))\n",
        "]).fit(X_train, y_train)\n",
        "\n",
        "# Predictions & scores\n",
        "y_pred = clf.predict(X_test)\n",
        "y_proba = clf.predict_proba(X_test)[:, 1] # for ROC/PR/thresholds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Accuracy (and why it can mislead on imbalance)\n",
        "\n",
        "If classes are imbalanced, \"majority-class always\" can look good, use more informative metrics."
      ],
      "metadata": {
        "id": "banAcz36VYmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = accuracy_score(y_test, y_pred)\n",
        "base_rate = (y_test == 0).mean()  # majority-class baseline accuracy\n",
        "print(f\"Accuracy: {acc:.3f}  | Majority baseline: {base_rate:.3f}\")"
      ],
      "metadata": {
        "id": "pftjyvSMVTdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Precision, Recall, F1 + Report\n",
        "\n",
        "When to prefer:\n",
        "* **Precision** when false positives are costly (e.g., auto-lock accounts).\n",
        "* **Recall** when missing positives is costly (e.g., fraud/attack detection).\n",
        "* **F1** when you need a single balance of precision & recall.\n",
        "\n",
        "Why useful: See where errors happen (FP vs FN) and quantify both class sides (specificity & sensitivity)."
      ],
      "metadata": {
        "id": "4qJR9VFmVp_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
        "disp = ConfusionMatrixDisplay(cm, display_labels=[\"Negative (0)\",\"Positive (1)\"])\n",
        "disp.plot(values_format=\"d\"); plt.title(\"Confusion Matrix\"); plt.show()\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "specificity = tn / (tn + fp)        # True Negative Rate\n",
        "sensitivity = tp / (tp + fn)        # Recall\n",
        "bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "print(f\"Specificity: {specificity:.3f}  Sensitivity/Recall: {sensitivity:.3f}  Balanced Acc: {bal_acc:.3f}\")\n"
      ],
      "metadata": {
        "id": "xzIHSQDzVi-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) ROC Curve & ROC-AUC (ranking quality across thresholds)\n",
        "\n",
        "Use when: Class imbalance is moderate and you care about ranking true positives higher across many thresholds."
      ],
      "metadata": {
        "id": "YzdueOCyV3au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auc_roc = roc_auc_score(y_test, y_proba)\n",
        "RocCurveDisplay.from_predictions(y_test, y_proba)\n",
        "plt.title(f\"ROC Curve (AUC = {auc_roc:.3f})\"); plt.show()"
      ],
      "metadata": {
        "id": "K3wLqMEAV7qA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5) Precision-Recall Curve & Average Precision (AP)\n",
        "\n",
        "Use when: Positive class is rare; PR captures performance where it matters (high recall regions)."
      ],
      "metadata": {
        "id": "Xpw_QQqPV3Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ap = average_precision_score(y_test, y_proba)\n",
        "PrecisionRecallDisplay.from_predictions(y_test, y_proba)\n",
        "plt.title(f\"Precision-Recall Curve (AP = {ap:.3f})\"); plt.show()\n"
      ],
      "metadata": {
        "id": "m1pjjxKbVy1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6) Threshold Tuning (trade precision vs recall)\n",
        "\n",
        "Moving the threshold changes FP/FN and aligns with business risk."
      ],
      "metadata": {
        "id": "o7DRhZlLWAzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thr_list = np.linspace(0.1, 0.9, 17)\n",
        "rows = []\n",
        "for thr in thr_list:\n",
        "    y_hat = (y_proba >= thr).astype(int)\n",
        "    rows.append({\n",
        "        \"threshold\": thr,\n",
        "        \"precision\": precision_score(y_test, y_hat, zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_hat, zero_division=0),\n",
        "        \"f1\": f1_score(y_test, y_hat, zero_division=0),\n",
        "        \"balanced_acc\": balanced_accuracy_score(y_test, y_hat)\n",
        "    })\n",
        "thr_df = pd.DataFrame(rows)\n",
        "thr_df.head()\n"
      ],
      "metadata": {
        "id": "yzJ4NN9rWAK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(7,4))\n",
        "ax.plot(thr_df[\"threshold\"], thr_df[\"precision\"], label=\"Precision\")\n",
        "ax.plot(thr_df[\"threshold\"], thr_df[\"recall\"],    label=\"Recall\")\n",
        "ax.plot(thr_df[\"threshold\"], thr_df[\"f1\"],        label=\"F1\")\n",
        "ax.set_xlabel(\"Decision Threshold\"); ax.set_ylabel(\"Score\"); ax.grid(True); ax.legend(); plt.show()\n",
        "\n",
        "# Pick operating point by business goal: e.g., target recall >= 0.85 with best precision\n",
        "target_recall = 0.85\n",
        "candidates = thr_df[thr_df[\"recall\"] >= target_recall].sort_values(\"precision\", ascending=False)\n",
        "candidates.head(3)\n"
      ],
      "metadata": {
        "id": "PGI7NelHWHIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7) Macro/Micro Averaging (quick note + example)\n",
        "\n",
        "For multi-class or highly imbalanced binary tasks, macro treats classes equally; micro aggregates globally."
      ],
      "metadata": {
        "id": "UkX5iRFdWJIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick illustration: compute macro/micro F1 via cross_validate (binary still OK)\n",
        "scores = cross_validate(\n",
        "    clf, X_train, y_train, cv=5,\n",
        "    scoring={\"f1_macro\":\"f1_macro\", \"f1_micro\":\"f1_micro\", \"f1\":\"f1\"}\n",
        ")\n",
        "{m: (scores[f\"test_{m}\"].mean(), scores[f\"test_{m}\"].std()) for m in [\"f1_macro\",\"f1_micro\",\"f1\"]}\n"
      ],
      "metadata": {
        "id": "FNWhI8NwWIIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) Multiple Metrics via cross_validate (one pass)\n",
        "\n",
        "Tip: Report mean ± std over folds to communicate stability."
      ],
      "metadata": {
        "id": "04GQiDH8WNc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scorers = {\n",
        "    \"acc\": \"accuracy\",\n",
        "    \"f1\": \"f1\",\n",
        "    \"rocauc\": \"roc_auc\",\n",
        "    \"avg_prec\": \"average_precision\",\n",
        "    \"bal_acc\": \"balanced_accuracy\"\n",
        "}\n",
        "cv = cross_validate(clf, X_train, y_train, cv=5, scoring=scorers, return_train_score=False)\n",
        "pd.DataFrame(cv).agg([\"mean\",\"std\"]).T\n"
      ],
      "metadata": {
        "id": "kgT7X7HVWMXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9) Putting It Together: Metric Selection\n",
        "\n",
        "* Balanced data & equal error costs: accuracy, F1.\n",
        "* Rare positives (imbalance): PR curve, Average Precision, recall/F1.\n",
        "* Screening (don’t miss positives): high recall, monitor FP.\n",
        "* Auto-action (avoid false alarms): high precision, accept lower recall.\n",
        "* Ranking scenarios: ROC-AUC.\n",
        "* Unequal class importance: balanced accuracy, specificity + sensitivity."
      ],
      "metadata": {
        "id": "xECe8kKMWS2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "You computed and interpreted: Accuracy, Precision, Recall, F1, Confusion Matrix, ROC-AUC, PR/AP, Balanced Accuracy, and performed threshold tuning & CV with multiple metrics. You can now select and justify the right metric for the business need and set an operating threshold that matches risk."
      ],
      "metadata": {
        "id": "QG6frw8oWVKj"
      }
    }
  ]
}